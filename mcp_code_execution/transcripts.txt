# Video 1 - https://www.youtube.com/watch?v=jJMbz-xziZI
Enthropic just published something about MCP that every single person building AI agents needs to hear. If you have been using model context protocol in production, you might have noticed your agents hallucinating more than they should. Your token costs spiraling out of control and workflows randomly breaking when they hit context limits. And here is what nobody is telling you. It's not because MCP is broken. It's because the way that everyone has been using MCP is fundamentally inefficient. Now, I'm talking about burning through 98% more tokens than you need to, and your agents are getting confused because their context is cluttered with hundreds of tool definitions they're never going to use. So, if you're building AI systems for clients or running these systems in production for your own business, this changes everything about what is actually reliable and profitable. Now, I have been building AI automations for businesses for 2 years now, and I've hit these exact problems on previous projects. context windows are maxing out, costs exploding to where the economics just do not work, and agents making just stupid mistakes because there's just too much noise. Now, what Enthropic just laid out, it isn't some new tool that you have to learn. It's a completely different way to think about your agents and how they interact with MCP servers and it solves all of these problems. So, let me just break down what this actually means for you. So, most of you by now know what MCP is. Model context protocol. This became the industry standard for connecting AI agents to any external tools and data sources. Now the genius of MCP is that you just build a server once and any agent can connect to it. So we have seen thousands of these servers get built over the last years. The whole ecosystem has exploded because finally we had a universal way to connect agents to anything like Gmail or Slack, databases, CRM, whatever it may be. So you build the MCP server and you're done. But here's the problem that hits you the moment that you start building anything complex for real clients and try to run it in production. Everything dumps into your agents context window and it becomes a complete mess. So let me give you an example from some of our own work. So we recently built a system for a legal client. Now, they needed their agent to just search case law and pull some of their documents from their document management system, check their internal management software and update some of their calendars, send client emails, and pretty much just log everything into their CRM. So, they had about six different systems, right? So, sounds pretty reasonable. Now, each MCP server, it had maybe 15 to 20 different tools that you can use. So, now we're looking at over a hundred different functions. And here was what was so detrimental is that even though the agent only uses maybe three or four of those tools for any specific task, all 100 tool definitions are logged into the context window from the very start. So every single function has its description. It has its required parameters, optional parameters, return types, examples. So what we're talking about is tens of thousands of tokens just sitting there before the agent even reads like what the user wants it to do. So immediately your costs are higher than they need to be. Your response times, they're going to be slower because the agent has to process all of this noise. And here's what I had noticed that really matters for production systems is that the agent, it makes more mistakes when there's just too much clutter in the context. So it gets confused about which tool to actually use. It hallucinates parameters that just do not exist. It tries to call tools in ways that do not make sense. And when you have a hundred tool definitions competing for attention, the agents accuracy, it just drops. And that's a massive problem when you're running this for any real clients who expect it to work reliably. But that's just the first problem. The second problem is even more brutal for your economics. So let's say your agent needs to grab a deposition transcript from the document system. That transcript, it might be 40,000 tokens. And with the traditional way of using MCP, that entire transcript, it loads into the agent's context. And then the agent needs to summarize key points and update the case file in the CRM. So now that same 40,000 token transcript, it's getting processed again as the agent just writes it into the next system. So you're literally paying for that same data to flow through your context multiple different times. And if you're chaining together several operations across different systems, well, you're just going to hit context window limits or just completely blow through your API budget before you even finish the workflow. Now, I've had projects where the token cost made the whole thing economically questionable. And that is before we even talk about the reliability issues from context limits before hitting any midworkflow. So, here is where code execution changes the entire game. And this is really just about understanding how AI models actually work best. So instead of presenting your MCP tools as function calls that the agent makes directly, you just present them as a file system that the agent can explore. So each MCP server it just becomes a folder and each tool within that server is a TypeScript file and the agent it can just search through the structure, find exactly what it needs and then write code to use those specific tools. And here's why this approach is so much more powerful and reliable. is that AI models, they're fundamentally trained on massive amounts of code during their pre-training phase. So, we're talking about millions and millions of lines of code. So, tool calling, that is something they learn during post-training with way less compute behind it. So, when you let the agent write code to actually interact with your MCP servers, you're just leaning into what the model is actually exceptional at instead of just forcing it into a more rigid structure that it is less naturally good at. So let me just walk you through how the workflow actually changes between these two approaches so that you can see the entire difference between these. So with the traditional approach, all of your tool definitions, they just load into the context window right from the start. And then the user, they ask for something. So that agent has to sort through all of that noise just to figure out which tools are even relevant to its goal. And then it calls tool A and it gets back maybe 40,000 tokens of data. That all goes into the context and then it needs to call tool B using some of that data. So another 30,000 tokens just flows through and your context window it's filling up fast and the agent is struggling to keep track of everything and compute the tasks without making any sort of mistakes. Now if you compare that to the code execution approach on the other hand the agent it has access to this organized file structure of your MCP servers and all of its respective tools. So a user they ask for something and then the agent it searches for the right tool folder and it finds what it actually needs and then it loads only that specific tool definition not every single tool from every server. So already it's way less noise and way less confusion and then it just writes codes to call that tool. So here's the big piece about this though is the results they stay in a sandbox variable just outside of the agent's context. So the agent can then write more code to filter that data, transform it and then extract, you know, just what actually matters and only the final processed result, maybe 500 tokens instead of 40,000 goes back into the agent context window. Now the agent, it never gets overwhelmed with massive amounts of data it does not need. So the difference it's absolutely massive for both reliability and also cost, which obviously is very important. So, the legal transcript example that I mentioned, instead of 40,000 tokens flowing through the contacts twice, you're talking about maybe 2,000 tokens total for the whole operation. And the agent, it does all the heavy data processing work in the sandbox environment where it can actually filter and transform and extract only what is relevant. And then it is going to bring back just the key information that it needs. And because the context stays clean, the agent makes way fewer mistakes. So think about it like this. The traditional MCP approach, it is like being forced to carry every single tool in your toolbox with you everywhere you go and having to read all the instruction manuals out loud before you can even use just one tool. Now, of course, you're going to get confused and you're going to pick the wrong tool sometimes. That's just going to happen. So code execution, it's just like having a workshop where you can walk over to the right section, grab exactly what you need, work on your project there, and only show, you know, people the finished results, so you're not cluttering up your workspace with everything all at once, so you can actually focus and do the work correctly. Now, really quick, I just wanted to mention that if you want to learn more about implementing advanced systems just like this and actually building a real profitable business around AI automation, join my school community. We have got over 15,000 members and they're actively sharing what is working right now in the real world and we give out all of our free resources with monthly competitions and so much more. Link is down below in the description. Again, it's completely free. All right, so beyond just the token savings and reliability improvements, let me tell you why this actually matters for your actual business or your client's businesses. So, first off, the economics of what you can build, it completely changes. So, I have been doing discovery calls and audits with potential clients constantly and when I sit down and actually calculate what it would cost to run the automation using traditional MCP, sometimes the numbers just do not make sense. So, a customer support agent that's handling 200 tickets per day and each ticket it requires pulling data from multiple different systems. Well, you could be looking at $400 to $600 per day in API cost alone with the traditional approach. But with code execution, I can get that down to maybe $40 to $60 per day. Now suddenly the return on investment, it actually makes sense and the client can afford to run this at scale without the cost, you know, spiraling out of control. And more importantly, the system it actually works reliably because the agent it is not getting confused by any of the cluttered context associated with it. Second is that you can finally build things that were literally impossible before because of cost constraints and reliability issues. So, here is a use case that I have wanted to build with MCP alone for months, but I just couldn't make the economics work or just trust it to at least run reliably. Now, this was just an agent for an e-commerce brand that monitors inventory levels across Shopify, Amazon, their third party logistics warehouse system, and also some of their accounting software. And then what it would do is identify the discrepancies between these systems and then flag any potential stockouts before they happen and just suggest specific reorder quantities for each SKU. So with traditional MCP each data pull from these systems, it was very large. So comparing data across multiple different sources, this obviously means multiple passes through your context window. Now the token cost, this would just be completely insane and unsustainable. And with all of that data flowing through context, the chances of the agent making a mistake or hallucinating something, it increases dramatically. But with code execution, the agent, it's just going to pull everything into the sandbox and then it's going to write a comparison script and then run all of the calculations there. And it only returns something like SKU123. It's 47 units short in Amazon versus your 3PL system. Here is the recommended reorder action. So maybe a,000 tokens instead of 150,000. And because the context it stays clean, it's way more reliable. So that completely changes what is feasible to build and actually deploy in production. Third is privacy. It becomes a massive selling point instead of a deal breakaker. Now I have personally lost deals because enterprise clients absolutely will not allow their customer data to touch enthropic or open AAI servers. So healthcare companies, financial services, law firms, they all have strict compliance requirements like HIPPA for example. So with code execution sensitive data, it never actually goes to the model. It just stays in the sandbox environment. So you can even set up automatic tokenization where the model sees something like customer emil1 instead of the actual email address. Now, the real data, it just flows from system A to system B, but the AI never actually reads the sensitive parts. So, for regulated industries, this just unlocks deals that you literally could not touch before because of compliance issues. And fourth, this one's honestly kind of wild, is the agent can actually learn and improve over time. So, because the agent is working in a file system, it can save useful code that it writes. So, let's say it just figures out a really clever way to parse a specific document format. So it can save that as a reusable function and use it just again later. So over time your agent it builds up its own library of solutions. So it's not starting from scratch every single time. And this is very similar to how claude skills works. So the agent literally evolves its own capabilities. Now, before we get into the practical implications, I just wanted to mention that if you are a business owner looking for help implementing this or just to transform your business with AI to ultimately increase your bottom line and save your team hours every single week and actually grow and get an edge over your competitors, then you can click on the link in the description to schedule in a call with our team just to learn more. Completely free. We've worked with more than 30 different companies now, either driving significant amounts of leverage or just increasing their bottom line and allowing them to scale efficiently. So, if you're at all interested in growing your company, then again, the link is down below in the description. But in any case, let's jump back into things. So, let's get really practical here and talk about what this means for different types of businesses and use cases. So, if you are running an agency or just doing consulting work, your entire pricing model, it just got way more flexible. So, I have been hesitant to propose certain automation projects to clients just because, you know, when I would run the numbers, the token costs made the ROI really questionable. And honestly, I wasn't confident that the system would be reliable enough for any production use with any traditional MCP methods. So, if a client needs an agent that processes 500 documents per day. So, with traditional MCP, I mean, you're just staring down massive ongoing API bills that eat into everyone's margins, plus the risk of the agent making mistakes because of context overload. So, code execution, it fundamentally changes that calculation. Now I can confidently promise much more ambitious automation projects just because I know that the cost will scale in a reasonable way and the system will actually work reliably and that changes what you can actually sell to your clients and how you price your services. All right. Now let me be completely honest about the downsides here because they definitely exist and you need to know about them. So first it's way less reliable in certain ways. So traditional MCP tool calling it's rigid but it's very predictable. So the agent, it calls a specific function with specific parameters. So it either works or it just throws away an error. Pretty straightforward code execution. This just means that the agent has to write systematically correct code every single time that it needs to do something. So that opens the doors to either syntax errors or logic bugs, edge cases that the agent just didn't account for. So, I've personally seen agents write code that works perfectly 19 times in a row and then just completely fails the 20th attempt because the data came back in a slightly different format than expected. So, you do need much better testing, error handling, and also monitoring system. It's also not as bulletproof as simple tool calling can be. So, second, the infrastructure overhead. It's very real. So you absolutely cannot just deploy this to a simple serverless function and then call it done because you need a proper sandbox environment that is secure that is isolated from your other systems and has strict limits on what code can actually do and what resources that it can consume and that is real DevOps work. So for a simple chatbot that does one or two different things that level of infrastructure it's just completely overkill. But for production systems that are handling actual business processes with real consequences, it's pretty much necessary. So it's just not trivial to set up and maintain. So when should you actually use each approach? Well, let me give you my framework for thinking about this. So traditional MCP, it still makes total sense for any simple use cases that only needs one or two or maybe three tool calls. You know, where it's low volume operations where token costs do not really matter in the grand scheme of things. So quick prototypes and MVPs where you need to move fast and prove the concept in situations where absolute reliability it matters more than cost optimization. So that is where traditional is going to be the better bet. [snorts] Now code execution. This makes way more sense for any complex workflows that involve any heavy data processing or transformation or high volume operations where costs can compound very quickly and really matter in enterprise clients who have strict privacy and compliance requirements or just workflows that keep hitting context window limits with the traditional approach. So these are just situations where you need the agent to handle any messy unpredictable data that does not always come in the same format and also production systems where you need the agent to actually be reliable and not hallucinate or make mistakes because of context overload. So here is my personal rule of thumb that I use when evaluating all my projects. If you can actually build the entire thing with under 10 tool calls and the data being passed around is relatively small, just stick with traditional MCP. Keep it simple. But if you're chaining together complex operations or just processing large amounts of data or if you need it to be bulletproof in production code execution it's absolutely worth the upfront investment in infrastructure and setup time. So that being said if you are building AI solutions for your clients or for your own business just understanding this right now it already gives you a massive head start over your competition. So those complex workflows that other agencies are turning down just because they can't make the economics work or don't believe in it or guarantee reliability, you can build those profitably now. Or those enterprise deals that just keep dying because of privacy and compliance concerns. Well, you can close those deals. But here's what I really want you to focus on is just stop obsessing over which tool or platform is the best. Code execution, it's just one approach. Traditional MCP, it's another one. So the real question is never which one is better in abstract. The question is which one actually solves your specific client's problem most efficiently given their constraints and their requirements. So that's just the thinking that separates people who make real money from people who just collect tools. So with that being said, go ahead and investigate for yourself. Let me know what you guys think down below in the comments. Go ahead and read the article. I put it down below in the description. But again, if you're a business owner looking to implement things like this for your own business or looking to any other AI solutions to implement to ultimately save time and increase your bottom line, then check out the link down below in the description. You can book in a call with my team. And with that being said, thank you guys for watching. and I'll see you in the next video

# Video 2 - 
Good morning/ afternoon everyone. Today I'm excited to share insights from Enthropics engineering team about a powerful approach to building more efficient AI agents using the model context protocol or MCP. The core idea we'll explore is simple but transformative. Instead of having agents make direct tool calls that consume massive amounts of context, we can have them write code to interact with MCP servers. This shift can reduce token usage by over 98% in some cases. dramatically improving both performance and cost efficiency. As AI agents scale to interact with hundreds or thousands of tools, this approach becomes not just beneficial but essential. Let's dive into why this matters and how it works. What is MCP? First, let me quickly explain what MCP is for those who may not be familiar. The model context protocol is an open standard for connecting AI agents to external systems. Before MCP, connecting agents to tools was a fragmented landscape. Each agent tool pairing required its own custom integration. If you had 10 different agents and wanted them to connect to 20 different tools, you'd potentially need 200 different integrations. This created enormous duplication of effort and made it nearly impossible to scale connected AI systems. MCP solves this by providing a universal protocol. It's like USB for AI agents. Implement the standard once and suddenly your agent can connect to an entire ecosystem of tools. Since Anthropic launched MCP in November 2024, adoption has been remarkable. The community has built thousands of MCP servers. SDKs are available for all major programming languages and the industry has essentially adopted MCP as the deacto standard for agent tool connections. This success has been exciting, but it's also exposed new scaling challenges, which is exactly what we're here to address today. The scaling problem. As developers build more ambitious agents, they're connecting them to hundreds or even thousands of tools across dozens of MCP servers. This is where we start to see critical efficiency issues emerge. There are two main problems that arise at scale. First is tool definition overload. The typical approach is to load all available tool definitions up front directly into the model's context window. Each tool definition includes descriptions, detailed parameter specifications with types and requirements, and information about what the tool returns. Now, this doesn't look that bad for two tools, but imagine this for a thousand tools or 10,000. Some enterprise agents are connecting to that many systems. Before the agent can even start thinking about the user's actual request, it has to process this massive wall of tool definitions. The impact is significant. Your agent is essentially reading a small book's worth of API documentation before it can do anything useful. This increases latency. Users have to wait longer for responses. It increases costs. You're paying for all those tokens. And it reduces the amount of context available for the actual task at hand. This is problem number one. And it only gets worse as you add more integrations. Problem two, intermediate results. Now, let's look at the second problem. How intermediate results consume context. Here's a real world scenario. A user asks their agent to download my meeting transcript from Google Drive and attach it to a Salesforce lead. Simple request, right? But look at what happens under the hood. First, the agent calls Google Drive to get the document. The full transcript, which could be tens of thousands of words, gets loaded into the model's context window. The model processes it. Then the model needs to call Salesforce to update the record. And to do that, it has to write out the entire transcript again as part of the tool call parameters. So that meeting transcript flows through the context window twice. For a two-hour sales meeting, we're talking about 50,000 additional tokens. You're essentially paying for the model to read and write the same content multiple times, even though the model doesn't actually need to reason about most of that content. It's just moving data from point A to point B. And this is just a simple two-step operation. Imagine a complex workflow with 10 or 20 steps, each passing large amounts of data through the model. The token usage balloons quickly. This is particularly problematic because these large intermediate results can also increase the likelihood of errors. Models might truncate data, make mistakes when copying, or simply run out of context space. Architecture comparison. Let me show you the traditional MCP architecture to make these problems crystal clear. The flow works like this. The MCP client loads all tool definitions into the context window. The LLM receives these definitions along with the user's request. When the LLM decides to use a tool, it makes a tool call. That call goes to MCP server, which executes it and returns the result. That result goes back to the LLM where it's added to the context. The LLM processes the result and potentially makes another tool call, continuing this loop. Every single operation in this flow passes through the context window. Every tool definition, every tool call, every result. The context window becomes this central bottleneck where everything has to flow through. Now for simple agents with a handful of tools, this works fine. But as you scale up, this architecture creates increasingly serious efficiency problems. The context window fills up with tool definitions and intermediate data, leaving less room for actual reasoning. Processing time increases, costs escalate, and you start hitting hard limits on what's even possible. This is the fundamental challenge we need to solve. And the solution, it turns out, comes from a different way of thinking about how agents interact with tools. The solution. So what's the solution? It's actually elegantly simple. Instead of presenting MCP servers as direct tool calls, we present them as code APIs. Then we let the agent write code to interact with those APIs. This might sound like a small change, but it fundamentally transforms the architecture and it addresses both of our efficiency problems simultaneously. Here are the key benefits of this approach. First, progressive disclosure. Instead of loading all tool definitions up front, the agent can discover and load only the tools it actually needs for the current task. It's like having a library where you only check out the books you need rather than carrying the entire library around with you. Second, context efficiency. Data can be processed in the code execution environment, not in the model's context. The agent can fetch large data sets, filter them, transform them, and only pass the final relevant results back to the model. Third, better control flow. With code, you can use familiar programming constructs like loops, conditionals, and error handling. This is much more efficient than chaining individual tool calls through the model. Fourth, privacy preservation. Sensitive data can flow through your workflow without ever entering the model's context window. Only the information you explicitly log or return becomes visible to the model. And fifth, state persistence. The code execution environment can maintain state across operations, allowing agents to build up context over time and even save reusable functions as skills. The core insight here is that LLMs are excellent at writing code. We should leverage that strength to build more efficient AI architectures. Implementation file tree. Let me show you how this works in practice. One elegant implementation approach is to generate a file tree representation of all available tools from your connected MCP servers. Here's what that looks like. You have a servers directory at the root. Inside you have subdirectories for each connected server, Google Drive, Salesforce, Slack, and so on. Within each server directory, you have individual TypeScript files for each tool, plus an index file. This structure is intuitive and navigable. The agent can explore it just like a developer would. Want to see what Google Drive tools are available? List the Google Drive directory. Want to understand what the get document function does? Read the get document.ts file. The beauty of this approach is progressive disclosure in action. The agent doesn't need to load all tool definitions up front. It can navigate the file system, discover what's available, and load only what it needs. For an agent connected to a thousand tools but only needing five for the current task, this means loading five tool definitions instead of a thousand. That's a 99.5% reduction in tool related token usage right there before we even start executing operations. Tool file structure. Now let's look at what an individual tool file contains. Here's the get document tool for Google Drive. It's a simple TypeScript file with clear type definitions. We define an input interface specifying what parameters the function takes. In this case, just a document ID. We define a response interface specifying what the function returns, the document content. Then we have the actual function. It's a simple async function that takes the typed input and calls call MCP tool under the hood which handles the actual MCP protocol communication. The function is documented with a comment and it's properly typed for excellent developer experience. This is clean, readable, and exactly what developers expect when working with APIs. The code can read this file and immediately understand how to use the tool, what it expects, and what it returns. This also makes it trivial to add new tools or modify existing ones. You're just editing TypeScript files, not complex agent configuration. The file tree itself becomes the source of truth for available capabilities. Code execution in action. Now, let me show you the dramatic difference this makes for actual agent operations. Remember our example from earlier? The user wants to download a meeting transcript from Google Drive and attach it to a Salesforce lead. Simple request. But look at what happens under the hood. First, the agent calls Google Drive to get the document. The full transcript, which could be tens of thousands of words, gets loaded into the model's context window. The model processes it. Then, the model needs to call Salesforce to update the record. And to do that, it has to write out the entire transcript again as part of the tool call parameters. So that meeting transcript flows through the model's context window twice. For a 2-hour sales meeting, we're talking about 50,000 additional tokens. You're essentially paying for the model to read and write the same content multiple times. Even though the model doesn't actually need to reason about most of that content, it's just moving data from point A to point B. And this is just a simple two-step operation. Imagine a complex workflow with 10 or 20 steps, each passing large amounts of data through the model. The token usage balloons quickly. Progressive disclosure. Let me dig deeper into each of these benefits. Starting with progressive disclosure. The key insight is that models are genuinely excellent at navigating file systems. They understand directory structures. They can list files. They can read files selectively. We can use these capabilities to implement smart ondemand tool loading. There are two main approaches. First, direct file system navigation. The agent can list the/servers directory to see what servers are available. If it sees Salesforce, it can list that directory to see what Salesforce tools exist. Then it can read specific tool files to understand their interfaces. The agent naturally explores the structure loading only what it needs. The second approach is to provide a search tools function. The agent can search for relevant tools by keyword. For example, searching for Salesforce returns all Salesforce related tools. You can even include a detail level parameter. Maybe the agent just wants tool names first, then descriptions, and only loads full schemas when it's ready to actually use a tool. Both approaches achieve the same goal. Instead of front-loading all tool definitions, we load them progressively as needed. For an agent connected to a thousand tools, maybe it only needs 10 for the current task. That's a 99% reduction in tool definition tokens. And here's the beautiful part. As agents become more sophisticated and develop better intuition about which tools they need, they get even better at this progressive loading, further improving efficiency over time. Context efficiency. The second major benefit is context efficient handling of tool results. Let me show you a powerful example. Imagine your agent needs to analyze a 10,000 row spreadsheet to find pending orders. With traditional tool calling, the agent would fetch the entire spreadsheet, all 10,000 rows, into its context window, then process it to find the pending ones. That's potentially hundreds of thousands of tokens for data that mostly gets filtered out. With code execution, the workflow is completely different. The agent writes code that fetches the spreadsheet into the execution environment, filters it right there using standard array operations, and then only logs a summary. The model sees found 47 pending orders and maybe the first five rows as examples. The other 9,953 rows never enter the model's context at all. This pattern works for all kinds of data operations, aggregations, joins across multiple data sources, extracting specific fields from large documents, computing statistics, anything where you need to process large amounts of data but only need to reason about a summary or subset. The execution environment becomes your data processing layer. The model writes the code that defines what processing should happen, but the heavy lifting happens outside the context window. Only the relevant results flow back to the model. This is particularly powerful for multi-step workflows. You can fetch data, process it, combine it with other data, process it again, and only present the final output to the model. Each intermediate step happens in the execution environment, preserving precious context space for actual reasoning. Control flow and privacy. Let me talk about two more critical benefits. Better control flow and privacy preservation. On the control flow side, code gives you access to all the powerful constructs you're used to in programming. Need to wait for something to happen? Write a while loop. Here's an example. Waiting for a deployment notification in Slack. The agent writes code that pulls the channel history every 5 seconds until it finds a message containing deployment complete. Implementing this with direct tool calls would be incredibly inefficient. You'd need alternating get channel history calls and sleep commands with the model processing results each time. With code, it's a simple loop that runs in the execution environment. The model only sees the final deployment notification received. The same principle applies to conditionals, error handling, retry logic, parallel operations, all the standard patterns of programming become available to your agent. This isn't just more efficient, it's also more reliable. You're using battle tested programming constructs rather than trying to orchestrate complex control flow through agent loops. Now, on the privacy side, code execution provides powerful guarantees. By default, intermediate results stay in the execution environment. The agent only sees what you explicitly log or return. If sensitive data flows through your workflow but never gets logged, it never enters the model's context. You can take this even further with automatic tokenization. The MCP client can intercept sensitive data like email addresses, phone numbers, or names and replace them with tokens before they reach the model. When that data is used in subsequent operations, the tokens get swapped back for the real values. The model never sees the actual sensitive information, but the workflow still executes correctly. This opens up whole new categories of use cases where you need to orchestrate operations involving sensitive data without exposing that data to the AI model itself. That's a powerful privacy guarantee. State persistence and skills. Let me talk about two more critical benefits. State persistence and the ability to develop skills. Because code execution typically includes file system access, agents can maintain state across operations, they can write intermediate results to files, enabling them to resume work after interruptions or track progress over longunning tasks. Here's a simple example. Querying a thousand leads from Salesforce and saving them as a CSV file in the workspace. Later, in a completely different execution, the agent can read that CSV back. The data persists between operations. But here's where it gets really interesting. Agents can also persist their code as reusable functions. Once an agent develops working code for a task, say converting a Google sheet to a CSV file, it can save that implementation as a skill for future use. The agent writes the function, saves it in a dot/skills directory, and can now import and use it in any future operation. Over time, the agent builds up a library of higher level capabilities. Instead of writing low-level tool orchestration code every time, it can compose existing skills to accomplish tasks more quickly and reliably. This ties into the broader concept of agent skills, folders of reusable instructions, scripts, and resources that improve model performance on specialized tasks. You can add skill.md files to document what each skill does and when to use it. The agent learns to recognize patterns where a particular skill is applicable. This is powerful because it means your agent gets better over time. Early on, it's writing everything from scratch. But as it develops more skills, it becomes faster and more capable. The system evolves and improves through use. Now, I should mention an important caveat. Code execution isn't free. Running agent generated code requires a secure execution environment with proper sandboxing, resource limits, and monitoring. These infrastructure requirements add operational overhead and security considerations that direct tool calls simply don't have. You need to carefully weigh the benefits. Reduced token costs, lower latency, improved tool composition, better privacy against these implementation costs. For simple agents with a handful of tools, the traditional approach might be perfectly fine. But as you scale up, the benefits of code execution become increasingly compelling. Implementation considerations. Let's talk about some practical considerations for implementing this approach. First, security is paramount. You're executing code generated by an AI model, which means you need robust sandboxing. The execution environment should be isolated with limited access to system resources. You need timeout mechanisms to prevent infinite loops. You need rate limiting and resource quotas. Anthropic has published detailed guidance on code sandboxing that you should definitely review. Second, think about your tool discovery mechanism. The filetree approach works great, but you might also want to provide search capabilities. A search tools function can help agents find relevant tools more quickly, especially when working with large numbers of integrations. Third, think about how you'll handle errors. Code can fail in many ways. Syntax errors, runtime errors, tool failures, timeout errors. Your agent harness needs to catch these errors, present them to the model in a useful way, and allow the agent to debug and retry. This is actually one of the advantages of code execution. Debugging a piece of code is often easier than debugging a complex chain of tool calls. Fourth, think about observability. You'll want logging and monitoring to understand what code your agents are running, what tools they're using, and where bottlenecks where failures occur. The code itself serves as a useful audit trail of what the agent attempted to do. And finally, consider how this approach integrates with your existing infrastructure. If you're already using MCP servers, adapting them for code execution is often straightforward. You're just changing how the agent interface is presented, not the underlying tool implementations. Real world results. Let me share some real world validation of this approach. Cloudflare has published their own findings on what they call code mode. Essentially the same technique we're discussing today. Their core insight aligns perfectly with ours. A IMLS are excellent at writing code and developers should take advantage of this strength to build more efficient agents. They've seen similar dramatic improvements in token efficiency and execution speed. Other organizations implementing this approach have reported comparable results. We're talking about 10x, 50x, even 100x improvements in token efficiency for certain workflows. The benefits compound as workflows become more complex. A simple two-step operation might see 90% token reduction, but a 20-step workflow involving large data processing might see 99% reduction. The more your agent needs to orchestrate, the more valuable code. Best practices. Based on our experience and community feedback, let me share some best practices for implementing code execution with MCP. First, start simple. Don't try to convert your entire agent system overnight. Pick a specific use case where you're hitting token limits or efficiency issues. Implement code execution for that and learn from the experience. Second, invest in good tool organization. A well ststructured file tree with clear naming conventions and good documentation makes a huge difference in how effectively agents can discover and use tools. Think of it as API design. Clarity and consistency matter. Third, provide good error messages. When code fails, the agent needs clear information about what went wrong and how to fix it. Good error handling in your tool implementations pays dividends in agent reliability. Fourth, implement progressive complexity. Start with basic tool definitions and allow agents to request more detailed information as needed. This natural information hierarchy improves efficiency without sacrificing capability. Fifth, monitor and iterate. Watch what code your agents are generating. Are they discovering tools efficiently? Are they writing clean, efficient code? Use these insights to improve your tool organization and documentation. Sixth, consider providing code examples. Just as API documentation benefits from examples, your agent benefits from seeing example code for common patterns. These examples can be included in tool files or in a separate examples directory. And finally, engage with the community. The MCP community is active on Discord and GitHub discussions. If you're implementing this approach, these are great places to ask questions, share your experiences, and learn from others who are solving similar problems. The protocol and practices are still evolving, and community input is invaluable. Looking forward, as we look to the future, I think code execution with MCP represents an important evolutionary step in how we build AI agents. We're moving from a paradigm where agents make individual tool calls, essentially functioning as very sophisticated API consumers to a paradigm where agents write programs that orchestrate multiple tools. This is a fundamental shift in capability and efficiency. As AI models continue to improve at code generation, this approach will become even more powerful. Models will write more sophisticated code, implement more complex control flow, and make better decisions about tool selection and data processing. We're also likely to see the emergence of agentspecific programming patterns and best practices. Just as we develop design patterns for object-oriented programming or microservices architectures, we'll develop patterns for agent code. How should agents structure error handling? What are the best ways to compose tools? How should state be managed across longunning operations? The concept of agent skills will mature. We'll see libraries of reusable skills emerge, potentially even a marketplace where developers can share well- tested implementations of common patterns. Agents will increasingly build on top of existing capabilities rather than starting from scratch each time. Privacy preserving computation will become more sophisticated. As organizations become more comfortable with AI agents handling sensitive data, the techniques for ensuring that data never enters model context will evolve and standardize. And finally, I expect we'll see convergence between traditional software engineering practices and agent development. The line between writing code and building an agent that writes code will blur. Developers will naturally think in terms of giving agents programming capabilities rather than just tool access. Community and resources. Before I wrap up, let me point you to resources if you want to explore this further. The MCP protocol has extensive documentation at modelcontextprotocol.io. This includes specifications, SDK documentation, and guides for building both servers and clients. The MCP GitHub repository at github.com/modelcontext protocol contains the core protocol implementation, SDKs for multiple languages, and thousands of community-built servers you can use or learn from. Anthropics Engineering blog, which is where the article we're discussing today was published, regularly features deep dives into agent architecture patterns and best practices. The cloud documentation also includes detailed guides on building effective agents. The MCP community is active on Discord in GitHub discussions. If you're implementing this approach, these are great places to ask questions, share your experiences, and learn from others who are solving similar problems. Cloudfare's blog post on code mode provides an independent validation of this approach with their own examples and insights. And if you're interested in the broader topic of AI agent architectures, there's a growing body of research and practical experience being shared across the industry. This is a rapidly evolving field and staying connected to the community is valuable. Summary and conclusion. Let me bring this all together. We started by understanding the challenge. As AI agents scale to interact with hundreds or thousands of tools via MCP, traditional direct tool calling creates serious efficiency problems. Tool definitions overload the context window and intermediate results consume excessive tokens. These aren't hypothetical issues. They're real bottlenecks limiting what we can build. The solution is elegantly simple. Present MCP servers as code APIs and let agents write code to interact with them. This approach leverages what AI models do best, generating code to solve efficiency problems in agent architectures. The benefits are substantial and multiaceted. Progressive disclosure means loading only needed tools. Context efficiency means processing data outside the context window. Better control flow means using established programming patterns. Privacy preservation means keeping sensitive data out of model contexts and state persistence means building up capabilities over time through skills. The real world results speak for themselves. 98% token reduction in some cases, dramatic cost savings, improved latency, and unlocked use cases that weren't previously feasible. But this isn't just about efficiency. It's about enabling a new generation of more capable, more reliable, and more scalable AI agent systems. As we move forward, code execution with MCP will likely become the standard approach for sophisticated AI agents. The technology is ready, the community is active, and the benefits are clear. If you're building AI agents that need to interact with multiple tools and systems, I encourage you to explore this approach. Thank you for your attention, and I'm happy to take questions.


# Video 3 



# Video 4 



# Video 5 

