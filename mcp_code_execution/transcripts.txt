# Video 0 - https://www.youtube.com/watch?v=Fg8VHXr-WVM - https://github.com/VRSEN/mcp-code-exec-agent (building up)
In my last video, I showed a new method proposed by Anthropic called MCP code execution. This technique is the future of building AI agents because it reduces token consumption 98% while giving your agents way more autonomy and flexibility to evolve their own skills. In this video, I'll build this life with you step by step. I'll compare this new approach with the old direct MCP method and give you the final repo plus a template to convert any MCP server to this new pattern. At the end, I'll also share whether this is actually ready for production or not. Before you watch this video, make sure you watch the previous one to fully understand all the theory behind this new method. Let's dive right in. So the agent that we will build today is the sales operations agent from Antropics blog post that can read meeting transcripts and then attach them into your CRM without actually reading the contents of the file. The architecture for this agent is pretty simple. It's just one agent with this code execution tool that connects to Google Drive MCP and Notion MCP because our CRM is in notion. So in order to get started, you need to copy our starter template which will now contain a new special command to use this new pattern. Make sure to set visibility to private and then open this repo in cursor. So I will be building this in cursor, but you can also use cloud codeex or any other AI coding agent that you prefer. The only thing you need to do if you're not using cursor is to simply tag this workflow file and the commands files that we'll be using in the prompt. So I'm going to start simple and just tell cloudset to create a sales ops agent with two built-in tools IPython interpreter and persistent shell tool. The reason we need these two tools is because the agent must be able to actually discover local files and then run them in Python in order to use this new approach. And both of these tools are already available in our framework. So let's hit send and wait until the agent is built. Now, I'm also going to tell it to add another sales ops direct MCP agent for comparison later. By the way, if you're building a more sophisticated MVP, there is also a new PRD command which you can use to have more control over your agent structure. Okay, now here's where the magic happens. I added a new special command called MCP code execution, which you can use to add MCP servers to an agent using this new pattern. So all you need to do in order to use this is simply tag this command and then tell cursor which MCP servers to use. So I recommend finding the exact MCP servers you need on GitHub or elsewhere and then adding links back into cursor. Now simply hit send again. So this command already describes to cursor pretty much everything it needs to know about this new pattern and even links the blog post and all the other resources that it might need. This way cursor can reliably add it using the same structure that they proposed in this blog post. So you can see how now cursor created the server directory with the code for our servers and then it proceeds to creating the code for individual tools. You might also need to authenticate the server if it's using or off. Now it proceeds to creating files for individual tools. So you can see how now this server is made here directly in code instead of passing this description and the arguments directly into the agents context window. Cursor saves this description in code. So then whenever our agent needs to use this tool, it will simply read this file and then it will see the whole description and the arguments in order to use this tool. So this way again we don't load all the tools. We only load one tool that cursor actually needs to use. Okay. So in total cursor created 15 tools for notion MCP server and four tools for Google drive. It has also already tested notion MCP server. However, it wasn't able to test Google Drive because of missing credentials. So make sure you provide all the credentials up front. Do not repeat my mistake. Typically you can find instructions on how to get the credentials inside the readme of your MCP server. Then you can simply tell cursor to retest the server. Then you will also be required to authenticate. Okay. Now the next step is to write instructions. So for writing instructions, I also recommend you use this write instructions command and then simply tag the agent that you want to create the instructions for. Then cursor is going to ask you a few questions to properly define the context for your agent. So make sure you answer all these questions carefully and the most important one is the process and workflow. So prompting is key with this technique and in order for this agent to use it effectively, it must clearly understand the process. So basically the first step is to always check available skills in the Mnt skills folder. I'll explain why Mnt folder later then it should use the skill if such a skill is found for a specific task. If no skills are found then it should only read one tool that it needs to use and after that it should combine all such tools in order to complete a task successfully and finally provide suggestions for new skills to be added. So you can play around with this workflow. This is just a general process from the blog post. But I think there are a lot of innovations just from prompting the agent. So let's hit send. And in the meantime, I also want to add MCPS using the direct MCP approach to the other sales ops direct MCP agent. So for this, we also have a command. It's simply add MCP. And then I'm also just going to link the two MCP servers that we added to the previous agent. Okay, now we are ready to test this agency. So let's open it in terminal and just run python agency.py. Pi. Now let's perform a simple test. Let me ask it what's on this notion page. So first the agent listed the mount skills directory. Then it read the fetch tool file to understand all the parameters. And it seems like it ran into some issues with the imports. However, then at the end I was still able to read this page by performing this fetch tool request and it provided me with the preview of this file. Okay. Now the next step is to deploy this agent and then we'll compare it with the direct MCP approach. So let's push our changes to GitHub and now let's go to agency. So hit create new agency and then find this new agent repo. Then provide your keys from the end file. And now let's wait until our agency is deployed. Okay. So now our agency is deployed. So let's actually test it. So let me put it in a custom GPT. Let's select both agents and let's deploy it. Now let me send this task from Andropics blog post to copy the transcript from Google Drive and paste it into a page in notion. Let's see if it's actually going to be able to do it. Seems like it did read way too many files. Like for example, it even read this server file which it definitely doesn't need. And now finally it tells me that it was in fact able to do this. So let's check the notion page. And yes indeed we now get this transcript from our Q&A call exactly like it was on Google Drive. And also at the end it proposed to create this new skill for itself. So let me tell it to save the skill. So you can see that the agent told me that it saved the skills in the mount skills directory. So the reason you need to use this mount directory is because we now have persistent storage on our platform. And without this feature, it would not be possible to allow your agents to evolve and build their own skills over time. So in this mount directory essentially your agents can now save files like this and reference them across different chats. So now if I start a new chat and then send the same prompt again, the agent should take a lot less time and just use the skill in order to do this task quickly. So you can see how now it found the skill and read the file and the agent was able to still perform the same task. Okay, now let's quickly test another agent that uses direct MCP approach. So now you can see how this agent is reading this whole transcript with the G drive read file tool which hopefully is not going to cost me too much and it also read the notion page and now it's trying to add this transcript to this notion page. But as you can see it has to manually type this whole thing which is definitely going to cost me too much now. So you can see how it's literally spending like probably tens of thousands of tokens for no reason simply because it had to do one simple copy paste operation. Finally, it's finished. So now let's take a look at the traces and analyze the costs. So tracing is enabled by default in our framework. And in order to see the traces, all you need to do is go to dashboard and then simply find the name of your agency. Okay, so let's see this sales ops direct MCP agent. And in total, this agent consumed 32,000 tokens. This is just insane. And most of these tokens aren't even input tokens. It's also a lot of output tokens, which are extremely expensive. And with this new approach, the amount of tokens is only 12,000, which is a lot less, although it's still quite a bit. And I think the reason this is still consumes so many tokens is because this agent, as you can see, just performs way too many unnecessary tool calls. But I do think that it's completely possible to optimize it with more prompting. And when the agent used one of the existing skills, it only consumed 4,000 tokens to perform the same task, which is now around 10 times less. So in conclusion, I believe this approach is extremely powerful, but as they say, with great power comes great responsibility. While I was definitely able to reduce the token consumption from the intermediate tool outputs, the agent still consumed more tokens than it should have when calling the tools. It executed code way too many times. It read way too many unnecessary files and it just took too long to perform a simple task on the first attempt. However, the agent was still able to save a lot more tokens compared to the old direct MCP approach. Additionally, it was even able to self-improve by creating a new skill which cut down the number of code executions in subsequent runs. So, my final verdict is yes, this approach is ready for production, but it requires proper prompting. Prompting here is key. While the LLMs aren't trained for this new method yet, you have to carefully describe how to use this new pattern, otherwise they're going to make a lot of mistakes. But if you do it right, and I'm sure many of you will, with this repo, the benefits far outweigh the costs. You get way more autonomy and flexibility with only a minor drop in reliability. So yes, this is the new paradigm. Just let your agents run the code. We no longer have to create these abstractions on top of other abstractions because agents can just generate the code to do whatever they need by themselves. However, don't use it for simple agents like customer support. save it for more sophisticated general agents like analytics, research, or operations. For simple agents, it still doesn't make sense. And when you're ready to deploy this, check out our platform because infrastructure overhead is actually the biggest downside of this approach. And we're the only platform on the market that supports everything you need to run this out of the box. More advanced EI agent templates are coming on this channel soon.

# Video 1 - https://www.youtube.com/watch?v=jJMbz-xziZI (https://www.anthropic.com/engineering/code-execution-with-mcp)
Enthropic just published something about MCP that every single person building AI agents needs to hear. If you have been using model context protocol in production, you might have noticed your agents hallucinating more than they should. Your token costs spiraling out of control and workflows randomly breaking when they hit context limits. And here is what nobody is telling you. It's not because MCP is broken. It's because the way that everyone has been using MCP is fundamentally inefficient. Now, I'm talking about burning through 98% more tokens than you need to, and your agents are getting confused because their context is cluttered with hundreds of tool definitions they're never going to use. So, if you're building AI systems for clients or running these systems in production for your own business, this changes everything about what is actually reliable and profitable. Now, I have been building AI automations for businesses for 2 years now, and I've hit these exact problems on previous projects. context windows are maxing out, costs exploding to where the economics just do not work, and agents making just stupid mistakes because there's just too much noise. Now, what Enthropic just laid out, it isn't some new tool that you have to learn. It's a completely different way to think about your agents and how they interact with MCP servers and it solves all of these problems. So, let me just break down what this actually means for you. So, most of you by now know what MCP is. Model context protocol. This became the industry standard for connecting AI agents to any external tools and data sources. Now the genius of MCP is that you just build a server once and any agent can connect to it. So we have seen thousands of these servers get built over the last years. The whole ecosystem has exploded because finally we had a universal way to connect agents to anything like Gmail or Slack, databases, CRM, whatever it may be. So you build the MCP server and you're done. But here's the problem that hits you the moment that you start building anything complex for real clients and try to run it in production. Everything dumps into your agents context window and it becomes a complete mess. So let me give you an example from some of our own work. So we recently built a system for a legal client. Now, they needed their agent to just search case law and pull some of their documents from their document management system, check their internal management software and update some of their calendars, send client emails, and pretty much just log everything into their CRM. So, they had about six different systems, right? So, sounds pretty reasonable. Now, each MCP server, it had maybe 15 to 20 different tools that you can use. So, now we're looking at over a hundred different functions. And here was what was so detrimental is that even though the agent only uses maybe three or four of those tools for any specific task, all 100 tool definitions are logged into the context window from the very start. So every single function has its description. It has its required parameters, optional parameters, return types, examples. So what we're talking about is tens of thousands of tokens just sitting there before the agent even reads like what the user wants it to do. So immediately your costs are higher than they need to be. Your response times, they're going to be slower because the agent has to process all of this noise. And here's what I had noticed that really matters for production systems is that the agent, it makes more mistakes when there's just too much clutter in the context. So it gets confused about which tool to actually use. It hallucinates parameters that just do not exist. It tries to call tools in ways that do not make sense. And when you have a hundred tool definitions competing for attention, the agents accuracy, it just drops. And that's a massive problem when you're running this for any real clients who expect it to work reliably. But that's just the first problem. The second problem is even more brutal for your economics. So let's say your agent needs to grab a deposition transcript from the document system. That transcript, it might be 40,000 tokens. And with the traditional way of using MCP, that entire transcript, it loads into the agent's context. And then the agent needs to summarize key points and update the case file in the CRM. So now that same 40,000 token transcript, it's getting processed again as the agent just writes it into the next system. So you're literally paying for that same data to flow through your context multiple different times. And if you're chaining together several operations across different systems, well, you're just going to hit context window limits or just completely blow through your API budget before you even finish the workflow. Now, I've had projects where the token cost made the whole thing economically questionable. And that is before we even talk about the reliability issues from context limits before hitting any midworkflow. So, here is where code execution changes the entire game. And this is really just about understanding how AI models actually work best. So instead of presenting your MCP tools as function calls that the agent makes directly, you just present them as a file system that the agent can explore. So each MCP server it just becomes a folder and each tool within that server is a TypeScript file and the agent it can just search through the structure, find exactly what it needs and then write code to use those specific tools. And here's why this approach is so much more powerful and reliable. is that AI models, they're fundamentally trained on massive amounts of code during their pre-training phase. So, we're talking about millions and millions of lines of code. So, tool calling, that is something they learn during post-training with way less compute behind it. So, when you let the agent write code to actually interact with your MCP servers, you're just leaning into what the model is actually exceptional at instead of just forcing it into a more rigid structure that it is less naturally good at. So let me just walk you through how the workflow actually changes between these two approaches so that you can see the entire difference between these. So with the traditional approach, all of your tool definitions, they just load into the context window right from the start. And then the user, they ask for something. So that agent has to sort through all of that noise just to figure out which tools are even relevant to its goal. And then it calls tool A and it gets back maybe 40,000 tokens of data. That all goes into the context and then it needs to call tool B using some of that data. So another 30,000 tokens just flows through and your context window it's filling up fast and the agent is struggling to keep track of everything and compute the tasks without making any sort of mistakes. Now if you compare that to the code execution approach on the other hand the agent it has access to this organized file structure of your MCP servers and all of its respective tools. So a user they ask for something and then the agent it searches for the right tool folder and it finds what it actually needs and then it loads only that specific tool definition not every single tool from every server. So already it's way less noise and way less confusion and then it just writes codes to call that tool. So here's the big piece about this though is the results they stay in a sandbox variable just outside of the agent's context. So the agent can then write more code to filter that data, transform it and then extract, you know, just what actually matters and only the final processed result, maybe 500 tokens instead of 40,000 goes back into the agent context window. Now the agent, it never gets overwhelmed with massive amounts of data it does not need. So the difference it's absolutely massive for both reliability and also cost, which obviously is very important. So, the legal transcript example that I mentioned, instead of 40,000 tokens flowing through the contacts twice, you're talking about maybe 2,000 tokens total for the whole operation. And the agent, it does all the heavy data processing work in the sandbox environment where it can actually filter and transform and extract only what is relevant. And then it is going to bring back just the key information that it needs. And because the context stays clean, the agent makes way fewer mistakes. So think about it like this. The traditional MCP approach, it is like being forced to carry every single tool in your toolbox with you everywhere you go and having to read all the instruction manuals out loud before you can even use just one tool. Now, of course, you're going to get confused and you're going to pick the wrong tool sometimes. That's just going to happen. So code execution, it's just like having a workshop where you can walk over to the right section, grab exactly what you need, work on your project there, and only show, you know, people the finished results, so you're not cluttering up your workspace with everything all at once, so you can actually focus and do the work correctly. Now, really quick, I just wanted to mention that if you want to learn more about implementing advanced systems just like this and actually building a real profitable business around AI automation, join my school community. We have got over 15,000 members and they're actively sharing what is working right now in the real world and we give out all of our free resources with monthly competitions and so much more. Link is down below in the description. Again, it's completely free. All right, so beyond just the token savings and reliability improvements, let me tell you why this actually matters for your actual business or your client's businesses. So, first off, the economics of what you can build, it completely changes. So, I have been doing discovery calls and audits with potential clients constantly and when I sit down and actually calculate what it would cost to run the automation using traditional MCP, sometimes the numbers just do not make sense. So, a customer support agent that's handling 200 tickets per day and each ticket it requires pulling data from multiple different systems. Well, you could be looking at $400 to $600 per day in API cost alone with the traditional approach. But with code execution, I can get that down to maybe $40 to $60 per day. Now suddenly the return on investment, it actually makes sense and the client can afford to run this at scale without the cost, you know, spiraling out of control. And more importantly, the system it actually works reliably because the agent it is not getting confused by any of the cluttered context associated with it. Second is that you can finally build things that were literally impossible before because of cost constraints and reliability issues. So, here is a use case that I have wanted to build with MCP alone for months, but I just couldn't make the economics work or just trust it to at least run reliably. Now, this was just an agent for an e-commerce brand that monitors inventory levels across Shopify, Amazon, their third party logistics warehouse system, and also some of their accounting software. And then what it would do is identify the discrepancies between these systems and then flag any potential stockouts before they happen and just suggest specific reorder quantities for each SKU. So with traditional MCP each data pull from these systems, it was very large. So comparing data across multiple different sources, this obviously means multiple passes through your context window. Now the token cost, this would just be completely insane and unsustainable. And with all of that data flowing through context, the chances of the agent making a mistake or hallucinating something, it increases dramatically. But with code execution, the agent, it's just going to pull everything into the sandbox and then it's going to write a comparison script and then run all of the calculations there. And it only returns something like SKU123. It's 47 units short in Amazon versus your 3PL system. Here is the recommended reorder action. So maybe a,000 tokens instead of 150,000. And because the context it stays clean, it's way more reliable. So that completely changes what is feasible to build and actually deploy in production. Third is privacy. It becomes a massive selling point instead of a deal breakaker. Now I have personally lost deals because enterprise clients absolutely will not allow their customer data to touch enthropic or open AAI servers. So healthcare companies, financial services, law firms, they all have strict compliance requirements like HIPPA for example. So with code execution sensitive data, it never actually goes to the model. It just stays in the sandbox environment. So you can even set up automatic tokenization where the model sees something like customer emil1 instead of the actual email address. Now, the real data, it just flows from system A to system B, but the AI never actually reads the sensitive parts. So, for regulated industries, this just unlocks deals that you literally could not touch before because of compliance issues. And fourth, this one's honestly kind of wild, is the agent can actually learn and improve over time. So, because the agent is working in a file system, it can save useful code that it writes. So, let's say it just figures out a really clever way to parse a specific document format. So it can save that as a reusable function and use it just again later. So over time your agent it builds up its own library of solutions. So it's not starting from scratch every single time. And this is very similar to how claude skills works. So the agent literally evolves its own capabilities. Now, before we get into the practical implications, I just wanted to mention that if you are a business owner looking for help implementing this or just to transform your business with AI to ultimately increase your bottom line and save your team hours every single week and actually grow and get an edge over your competitors, then you can click on the link in the description to schedule in a call with our team just to learn more. Completely free. We've worked with more than 30 different companies now, either driving significant amounts of leverage or just increasing their bottom line and allowing them to scale efficiently. So, if you're at all interested in growing your company, then again, the link is down below in the description. But in any case, let's jump back into things. So, let's get really practical here and talk about what this means for different types of businesses and use cases. So, if you are running an agency or just doing consulting work, your entire pricing model, it just got way more flexible. So, I have been hesitant to propose certain automation projects to clients just because, you know, when I would run the numbers, the token costs made the ROI really questionable. And honestly, I wasn't confident that the system would be reliable enough for any production use with any traditional MCP methods. So, if a client needs an agent that processes 500 documents per day. So, with traditional MCP, I mean, you're just staring down massive ongoing API bills that eat into everyone's margins, plus the risk of the agent making mistakes because of context overload. So, code execution, it fundamentally changes that calculation. Now I can confidently promise much more ambitious automation projects just because I know that the cost will scale in a reasonable way and the system will actually work reliably and that changes what you can actually sell to your clients and how you price your services. All right. Now let me be completely honest about the downsides here because they definitely exist and you need to know about them. So first it's way less reliable in certain ways. So traditional MCP tool calling it's rigid but it's very predictable. So the agent, it calls a specific function with specific parameters. So it either works or it just throws away an error. Pretty straightforward code execution. This just means that the agent has to write systematically correct code every single time that it needs to do something. So that opens the doors to either syntax errors or logic bugs, edge cases that the agent just didn't account for. So, I've personally seen agents write code that works perfectly 19 times in a row and then just completely fails the 20th attempt because the data came back in a slightly different format than expected. So, you do need much better testing, error handling, and also monitoring system. It's also not as bulletproof as simple tool calling can be. So, second, the infrastructure overhead. It's very real. So you absolutely cannot just deploy this to a simple serverless function and then call it done because you need a proper sandbox environment that is secure that is isolated from your other systems and has strict limits on what code can actually do and what resources that it can consume and that is real DevOps work. So for a simple chatbot that does one or two different things that level of infrastructure it's just completely overkill. But for production systems that are handling actual business processes with real consequences, it's pretty much necessary. So it's just not trivial to set up and maintain. So when should you actually use each approach? Well, let me give you my framework for thinking about this. So traditional MCP, it still makes total sense for any simple use cases that only needs one or two or maybe three tool calls. You know, where it's low volume operations where token costs do not really matter in the grand scheme of things. So quick prototypes and MVPs where you need to move fast and prove the concept in situations where absolute reliability it matters more than cost optimization. So that is where traditional is going to be the better bet. [snorts] Now code execution. This makes way more sense for any complex workflows that involve any heavy data processing or transformation or high volume operations where costs can compound very quickly and really matter in enterprise clients who have strict privacy and compliance requirements or just workflows that keep hitting context window limits with the traditional approach. So these are just situations where you need the agent to handle any messy unpredictable data that does not always come in the same format and also production systems where you need the agent to actually be reliable and not hallucinate or make mistakes because of context overload. So here is my personal rule of thumb that I use when evaluating all my projects. If you can actually build the entire thing with under 10 tool calls and the data being passed around is relatively small, just stick with traditional MCP. Keep it simple. But if you're chaining together complex operations or just processing large amounts of data or if you need it to be bulletproof in production code execution it's absolutely worth the upfront investment in infrastructure and setup time. So that being said if you are building AI solutions for your clients or for your own business just understanding this right now it already gives you a massive head start over your competition. So those complex workflows that other agencies are turning down just because they can't make the economics work or don't believe in it or guarantee reliability, you can build those profitably now. Or those enterprise deals that just keep dying because of privacy and compliance concerns. Well, you can close those deals. But here's what I really want you to focus on is just stop obsessing over which tool or platform is the best. Code execution, it's just one approach. Traditional MCP, it's another one. So the real question is never which one is better in abstract. The question is which one actually solves your specific client's problem most efficiently given their constraints and their requirements. So that's just the thinking that separates people who make real money from people who just collect tools. So with that being said, go ahead and investigate for yourself. Let me know what you guys think down below in the comments. Go ahead and read the article. I put it down below in the description. But again, if you're a business owner looking to implement things like this for your own business or looking to any other AI solutions to implement to ultimately save time and increase your bottom line, then check out the link down below in the description. You can book in a call with my team. And with that being said, thank you guys for watching. and I'll see you in the next video

# Video 2 - https://www.youtube.com/watch?v=RiUzfvC1m9s
Good morning/ afternoon everyone. Today I'm excited to share insights from Enthropics engineering team about a powerful approach to building more efficient AI agents using the model context protocol or MCP. The core idea we'll explore is simple but transformative. Instead of having agents make direct tool calls that consume massive amounts of context, we can have them write code to interact with MCP servers. This shift can reduce token usage by over 98% in some cases. dramatically improving both performance and cost efficiency. As AI agents scale to interact with hundreds or thousands of tools, this approach becomes not just beneficial but essential. Let's dive into why this matters and how it works. What is MCP? First, let me quickly explain what MCP is for those who may not be familiar. The model context protocol is an open standard for connecting AI agents to external systems. Before MCP, connecting agents to tools was a fragmented landscape. Each agent tool pairing required its own custom integration. If you had 10 different agents and wanted them to connect to 20 different tools, you'd potentially need 200 different integrations. This created enormous duplication of effort and made it nearly impossible to scale connected AI systems. MCP solves this by providing a universal protocol. It's like USB for AI agents. Implement the standard once and suddenly your agent can connect to an entire ecosystem of tools. Since Anthropic launched MCP in November 2024, adoption has been remarkable. The community has built thousands of MCP servers. SDKs are available for all major programming languages and the industry has essentially adopted MCP as the deacto standard for agent tool connections. This success has been exciting, but it's also exposed new scaling challenges, which is exactly what we're here to address today. The scaling problem. As developers build more ambitious agents, they're connecting them to hundreds or even thousands of tools across dozens of MCP servers. This is where we start to see critical efficiency issues emerge. There are two main problems that arise at scale. First is tool definition overload. The typical approach is to load all available tool definitions up front directly into the model's context window. Each tool definition includes descriptions, detailed parameter specifications with types and requirements, and information about what the tool returns. Now, this doesn't look that bad for two tools, but imagine this for a thousand tools or 10,000. Some enterprise agents are connecting to that many systems. Before the agent can even start thinking about the user's actual request, it has to process this massive wall of tool definitions. The impact is significant. Your agent is essentially reading a small book's worth of API documentation before it can do anything useful. This increases latency. Users have to wait longer for responses. It increases costs. You're paying for all those tokens. And it reduces the amount of context available for the actual task at hand. This is problem number one. And it only gets worse as you add more integrations. Problem two, intermediate results. Now, let's look at the second problem. How intermediate results consume context. Here's a real world scenario. A user asks their agent to download my meeting transcript from Google Drive and attach it to a Salesforce lead. Simple request, right? But look at what happens under the hood. First, the agent calls Google Drive to get the document. The full transcript, which could be tens of thousands of words, gets loaded into the model's context window. The model processes it. Then the model needs to call Salesforce to update the record. And to do that, it has to write out the entire transcript again as part of the tool call parameters. So that meeting transcript flows through the context window twice. For a two-hour sales meeting, we're talking about 50,000 additional tokens. You're essentially paying for the model to read and write the same content multiple times, even though the model doesn't actually need to reason about most of that content. It's just moving data from point A to point B. And this is just a simple two-step operation. Imagine a complex workflow with 10 or 20 steps, each passing large amounts of data through the model. The token usage balloons quickly. This is particularly problematic because these large intermediate results can also increase the likelihood of errors. Models might truncate data, make mistakes when copying, or simply run out of context space. Architecture comparison. Let me show you the traditional MCP architecture to make these problems crystal clear. The flow works like this. The MCP client loads all tool definitions into the context window. The LLM receives these definitions along with the user's request. When the LLM decides to use a tool, it makes a tool call. That call goes to MCP server, which executes it and returns the result. That result goes back to the LLM where it's added to the context. The LLM processes the result and potentially makes another tool call, continuing this loop. Every single operation in this flow passes through the context window. Every tool definition, every tool call, every result. The context window becomes this central bottleneck where everything has to flow through. Now for simple agents with a handful of tools, this works fine. But as you scale up, this architecture creates increasingly serious efficiency problems. The context window fills up with tool definitions and intermediate data, leaving less room for actual reasoning. Processing time increases, costs escalate, and you start hitting hard limits on what's even possible. This is the fundamental challenge we need to solve. And the solution, it turns out, comes from a different way of thinking about how agents interact with tools. The solution. So what's the solution? It's actually elegantly simple. Instead of presenting MCP servers as direct tool calls, we present them as code APIs. Then we let the agent write code to interact with those APIs. This might sound like a small change, but it fundamentally transforms the architecture and it addresses both of our efficiency problems simultaneously. Here are the key benefits of this approach. First, progressive disclosure. Instead of loading all tool definitions up front, the agent can discover and load only the tools it actually needs for the current task. It's like having a library where you only check out the books you need rather than carrying the entire library around with you. Second, context efficiency. Data can be processed in the code execution environment, not in the model's context. The agent can fetch large data sets, filter them, transform them, and only pass the final relevant results back to the model. Third, better control flow. With code, you can use familiar programming constructs like loops, conditionals, and error handling. This is much more efficient than chaining individual tool calls through the model. Fourth, privacy preservation. Sensitive data can flow through your workflow without ever entering the model's context window. Only the information you explicitly log or return becomes visible to the model. And fifth, state persistence. The code execution environment can maintain state across operations, allowing agents to build up context over time and even save reusable functions as skills. The core insight here is that LLMs are excellent at writing code. We should leverage that strength to build more efficient AI architectures. Implementation file tree. Let me show you how this works in practice. One elegant implementation approach is to generate a file tree representation of all available tools from your connected MCP servers. Here's what that looks like. You have a servers directory at the root. Inside you have subdirectories for each connected server, Google Drive, Salesforce, Slack, and so on. Within each server directory, you have individual TypeScript files for each tool, plus an index file. This structure is intuitive and navigable. The agent can explore it just like a developer would. Want to see what Google Drive tools are available? List the Google Drive directory. Want to understand what the get document function does? Read the get document.ts file. The beauty of this approach is progressive disclosure in action. The agent doesn't need to load all tool definitions up front. It can navigate the file system, discover what's available, and load only what it needs. For an agent connected to a thousand tools but only needing five for the current task, this means loading five tool definitions instead of a thousand. That's a 99.5% reduction in tool related token usage right there before we even start executing operations. Tool file structure. Now let's look at what an individual tool file contains. Here's the get document tool for Google Drive. It's a simple TypeScript file with clear type definitions. We define an input interface specifying what parameters the function takes. In this case, just a document ID. We define a response interface specifying what the function returns, the document content. Then we have the actual function. It's a simple async function that takes the typed input and calls call MCP tool under the hood which handles the actual MCP protocol communication. The function is documented with a comment and it's properly typed for excellent developer experience. This is clean, readable, and exactly what developers expect when working with APIs. The code can read this file and immediately understand how to use the tool, what it expects, and what it returns. This also makes it trivial to add new tools or modify existing ones. You're just editing TypeScript files, not complex agent configuration. The file tree itself becomes the source of truth for available capabilities. Code execution in action. Now, let me show you the dramatic difference this makes for actual agent operations. Remember our example from earlier? The user wants to download a meeting transcript from Google Drive and attach it to a Salesforce lead. Simple request. But look at what happens under the hood. First, the agent calls Google Drive to get the document. The full transcript, which could be tens of thousands of words, gets loaded into the model's context window. The model processes it. Then, the model needs to call Salesforce to update the record. And to do that, it has to write out the entire transcript again as part of the tool call parameters. So that meeting transcript flows through the model's context window twice. For a 2-hour sales meeting, we're talking about 50,000 additional tokens. You're essentially paying for the model to read and write the same content multiple times. Even though the model doesn't actually need to reason about most of that content, it's just moving data from point A to point B. And this is just a simple two-step operation. Imagine a complex workflow with 10 or 20 steps, each passing large amounts of data through the model. The token usage balloons quickly. Progressive disclosure. Let me dig deeper into each of these benefits. Starting with progressive disclosure. The key insight is that models are genuinely excellent at navigating file systems. They understand directory structures. They can list files. They can read files selectively. We can use these capabilities to implement smart ondemand tool loading. There are two main approaches. First, direct file system navigation. The agent can list the/servers directory to see what servers are available. If it sees Salesforce, it can list that directory to see what Salesforce tools exist. Then it can read specific tool files to understand their interfaces. The agent naturally explores the structure loading only what it needs. The second approach is to provide a search tools function. The agent can search for relevant tools by keyword. For example, searching for Salesforce returns all Salesforce related tools. You can even include a detail level parameter. Maybe the agent just wants tool names first, then descriptions, and only loads full schemas when it's ready to actually use a tool. Both approaches achieve the same goal. Instead of front-loading all tool definitions, we load them progressively as needed. For an agent connected to a thousand tools, maybe it only needs 10 for the current task. That's a 99% reduction in tool definition tokens. And here's the beautiful part. As agents become more sophisticated and develop better intuition about which tools they need, they get even better at this progressive loading, further improving efficiency over time. Context efficiency. The second major benefit is context efficient handling of tool results. Let me show you a powerful example. Imagine your agent needs to analyze a 10,000 row spreadsheet to find pending orders. With traditional tool calling, the agent would fetch the entire spreadsheet, all 10,000 rows, into its context window, then process it to find the pending ones. That's potentially hundreds of thousands of tokens for data that mostly gets filtered out. With code execution, the workflow is completely different. The agent writes code that fetches the spreadsheet into the execution environment, filters it right there using standard array operations, and then only logs a summary. The model sees found 47 pending orders and maybe the first five rows as examples. The other 9,953 rows never enter the model's context at all. This pattern works for all kinds of data operations, aggregations, joins across multiple data sources, extracting specific fields from large documents, computing statistics, anything where you need to process large amounts of data but only need to reason about a summary or subset. The execution environment becomes your data processing layer. The model writes the code that defines what processing should happen, but the heavy lifting happens outside the context window. Only the relevant results flow back to the model. This is particularly powerful for multi-step workflows. You can fetch data, process it, combine it with other data, process it again, and only present the final output to the model. Each intermediate step happens in the execution environment, preserving precious context space for actual reasoning. Control flow and privacy. Let me talk about two more critical benefits. Better control flow and privacy preservation. On the control flow side, code gives you access to all the powerful constructs you're used to in programming. Need to wait for something to happen? Write a while loop. Here's an example. Waiting for a deployment notification in Slack. The agent writes code that pulls the channel history every 5 seconds until it finds a message containing deployment complete. Implementing this with direct tool calls would be incredibly inefficient. You'd need alternating get channel history calls and sleep commands with the model processing results each time. With code, it's a simple loop that runs in the execution environment. The model only sees the final deployment notification received. The same principle applies to conditionals, error handling, retry logic, parallel operations, all the standard patterns of programming become available to your agent. This isn't just more efficient, it's also more reliable. You're using battle tested programming constructs rather than trying to orchestrate complex control flow through agent loops. Now, on the privacy side, code execution provides powerful guarantees. By default, intermediate results stay in the execution environment. The agent only sees what you explicitly log or return. If sensitive data flows through your workflow but never gets logged, it never enters the model's context. You can take this even further with automatic tokenization. The MCP client can intercept sensitive data like email addresses, phone numbers, or names and replace them with tokens before they reach the model. When that data is used in subsequent operations, the tokens get swapped back for the real values. The model never sees the actual sensitive information, but the workflow still executes correctly. This opens up whole new categories of use cases where you need to orchestrate operations involving sensitive data without exposing that data to the AI model itself. That's a powerful privacy guarantee. State persistence and skills. Let me talk about two more critical benefits. State persistence and the ability to develop skills. Because code execution typically includes file system access, agents can maintain state across operations, they can write intermediate results to files, enabling them to resume work after interruptions or track progress over longunning tasks. Here's a simple example. Querying a thousand leads from Salesforce and saving them as a CSV file in the workspace. Later, in a completely different execution, the agent can read that CSV back. The data persists between operations. But here's where it gets really interesting. Agents can also persist their code as reusable functions. Once an agent develops working code for a task, say converting a Google sheet to a CSV file, it can save that implementation as a skill for future use. The agent writes the function, saves it in a dot/skills directory, and can now import and use it in any future operation. Over time, the agent builds up a library of higher level capabilities. Instead of writing low-level tool orchestration code every time, it can compose existing skills to accomplish tasks more quickly and reliably. This ties into the broader concept of agent skills, folders of reusable instructions, scripts, and resources that improve model performance on specialized tasks. You can add skill.md files to document what each skill does and when to use it. The agent learns to recognize patterns where a particular skill is applicable. This is powerful because it means your agent gets better over time. Early on, it's writing everything from scratch. But as it develops more skills, it becomes faster and more capable. The system evolves and improves through use. Now, I should mention an important caveat. Code execution isn't free. Running agent generated code requires a secure execution environment with proper sandboxing, resource limits, and monitoring. These infrastructure requirements add operational overhead and security considerations that direct tool calls simply don't have. You need to carefully weigh the benefits. Reduced token costs, lower latency, improved tool composition, better privacy against these implementation costs. For simple agents with a handful of tools, the traditional approach might be perfectly fine. But as you scale up, the benefits of code execution become increasingly compelling. Implementation considerations. Let's talk about some practical considerations for implementing this approach. First, security is paramount. You're executing code generated by an AI model, which means you need robust sandboxing. The execution environment should be isolated with limited access to system resources. You need timeout mechanisms to prevent infinite loops. You need rate limiting and resource quotas. Anthropic has published detailed guidance on code sandboxing that you should definitely review. Second, think about your tool discovery mechanism. The filetree approach works great, but you might also want to provide search capabilities. A search tools function can help agents find relevant tools more quickly, especially when working with large numbers of integrations. Third, think about how you'll handle errors. Code can fail in many ways. Syntax errors, runtime errors, tool failures, timeout errors. Your agent harness needs to catch these errors, present them to the model in a useful way, and allow the agent to debug and retry. This is actually one of the advantages of code execution. Debugging a piece of code is often easier than debugging a complex chain of tool calls. Fourth, think about observability. You'll want logging and monitoring to understand what code your agents are running, what tools they're using, and where bottlenecks where failures occur. The code itself serves as a useful audit trail of what the agent attempted to do. And finally, consider how this approach integrates with your existing infrastructure. If you're already using MCP servers, adapting them for code execution is often straightforward. You're just changing how the agent interface is presented, not the underlying tool implementations. Real world results. Let me share some real world validation of this approach. Cloudflare has published their own findings on what they call code mode. Essentially the same technique we're discussing today. Their core insight aligns perfectly with ours. A IMLS are excellent at writing code and developers should take advantage of this strength to build more efficient agents. They've seen similar dramatic improvements in token efficiency and execution speed. Other organizations implementing this approach have reported comparable results. We're talking about 10x, 50x, even 100x improvements in token efficiency for certain workflows. The benefits compound as workflows become more complex. A simple two-step operation might see 90% token reduction, but a 20-step workflow involving large data processing might see 99% reduction. The more your agent needs to orchestrate, the more valuable code. Best practices. Based on our experience and community feedback, let me share some best practices for implementing code execution with MCP. First, start simple. Don't try to convert your entire agent system overnight. Pick a specific use case where you're hitting token limits or efficiency issues. Implement code execution for that and learn from the experience. Second, invest in good tool organization. A well ststructured file tree with clear naming conventions and good documentation makes a huge difference in how effectively agents can discover and use tools. Think of it as API design. Clarity and consistency matter. Third, provide good error messages. When code fails, the agent needs clear information about what went wrong and how to fix it. Good error handling in your tool implementations pays dividends in agent reliability. Fourth, implement progressive complexity. Start with basic tool definitions and allow agents to request more detailed information as needed. This natural information hierarchy improves efficiency without sacrificing capability. Fifth, monitor and iterate. Watch what code your agents are generating. Are they discovering tools efficiently? Are they writing clean, efficient code? Use these insights to improve your tool organization and documentation. Sixth, consider providing code examples. Just as API documentation benefits from examples, your agent benefits from seeing example code for common patterns. These examples can be included in tool files or in a separate examples directory. And finally, engage with the community. The MCP community is active on Discord and GitHub discussions. If you're implementing this approach, these are great places to ask questions, share your experiences, and learn from others who are solving similar problems. The protocol and practices are still evolving, and community input is invaluable. Looking forward, as we look to the future, I think code execution with MCP represents an important evolutionary step in how we build AI agents. We're moving from a paradigm where agents make individual tool calls, essentially functioning as very sophisticated API consumers to a paradigm where agents write programs that orchestrate multiple tools. This is a fundamental shift in capability and efficiency. As AI models continue to improve at code generation, this approach will become even more powerful. Models will write more sophisticated code, implement more complex control flow, and make better decisions about tool selection and data processing. We're also likely to see the emergence of agentspecific programming patterns and best practices. Just as we develop design patterns for object-oriented programming or microservices architectures, we'll develop patterns for agent code. How should agents structure error handling? What are the best ways to compose tools? How should state be managed across longunning operations? The concept of agent skills will mature. We'll see libraries of reusable skills emerge, potentially even a marketplace where developers can share well- tested implementations of common patterns. Agents will increasingly build on top of existing capabilities rather than starting from scratch each time. Privacy preserving computation will become more sophisticated. As organizations become more comfortable with AI agents handling sensitive data, the techniques for ensuring that data never enters model context will evolve and standardize. And finally, I expect we'll see convergence between traditional software engineering practices and agent development. The line between writing code and building an agent that writes code will blur. Developers will naturally think in terms of giving agents programming capabilities rather than just tool access. Community and resources. Before I wrap up, let me point you to resources if you want to explore this further. The MCP protocol has extensive documentation at modelcontextprotocol.io. This includes specifications, SDK documentation, and guides for building both servers and clients. The MCP GitHub repository at github.com/modelcontext protocol contains the core protocol implementation, SDKs for multiple languages, and thousands of community-built servers you can use or learn from. Anthropics Engineering blog, which is where the article we're discussing today was published, regularly features deep dives into agent architecture patterns and best practices. The cloud documentation also includes detailed guides on building effective agents. The MCP community is active on Discord in GitHub discussions. If you're implementing this approach, these are great places to ask questions, share your experiences, and learn from others who are solving similar problems. Cloudfare's blog post on code mode provides an independent validation of this approach with their own examples and insights. And if you're interested in the broader topic of AI agent architectures, there's a growing body of research and practical experience being shared across the industry. This is a rapidly evolving field and staying connected to the community is valuable. Summary and conclusion. Let me bring this all together. We started by understanding the challenge. As AI agents scale to interact with hundreds or thousands of tools via MCP, traditional direct tool calling creates serious efficiency problems. Tool definitions overload the context window and intermediate results consume excessive tokens. These aren't hypothetical issues. They're real bottlenecks limiting what we can build. The solution is elegantly simple. Present MCP servers as code APIs and let agents write code to interact with them. This approach leverages what AI models do best, generating code to solve efficiency problems in agent architectures. The benefits are substantial and multiaceted. Progressive disclosure means loading only needed tools. Context efficiency means processing data outside the context window. Better control flow means using established programming patterns. Privacy preservation means keeping sensitive data out of model contexts and state persistence means building up capabilities over time through skills. The real world results speak for themselves. 98% token reduction in some cases, dramatic cost savings, improved latency, and unlocked use cases that weren't previously feasible. But this isn't just about efficiency. It's about enabling a new generation of more capable, more reliable, and more scalable AI agent systems. As we move forward, code execution with MCP will likely become the standard approach for sophisticated AI agents. The technology is ready, the community is active, and the benefits are clear. If you're building AI agents that need to interact with multiple tools and systems, I encourage you to explore this approach. Thank you for your attention, and I'm happy to take questions.


# Video 3 - https://www.youtube.com/watch?v=7ad_1mC-zCk - (advantange and disadvantage) 
In my last MCP's video, we saw why classic MCP servers were their own abstraction for serious agents. They were eating like 50 to 100k tokens just on tool definitions. And the idea that we explored was basically just let your agents run the code. Let your agents generate the code for the tools that they actually need to use and lock only the results that they actually need to see. Antropic has now taken this exact idea and baked it directly into Clot. They just released what they call advanced tool use, a tool search tool, so Claude can discover tools on demand instead of loading everything up front. Programmatic tool calling made specifically for this new technique and tool use examples. So it learns to actually use your APIs from real examples, not just JSON schemas. This is amazing for us builders, but if you point it at the real data without the right safeguards, it can actually cause a lot more damage a lot faster. So, in this video, I want to do two things. Walk you through this new anthropic blog post and break down what actually changed in tool use and how it updates the advice from the end of MCP video. plus a quick look at how I apply the same pattern in my own agent framework even with other model providers that do not support this directly in their API yet. Let's dive right in. All right, so let's start with what exactly anthropic shipped here and how it connects to everything we talked about in the MCP video. In that video, I basically said that MCP is great as a standard, but the way everyone is using it right now is kind of broken. You connect a bunch of MCP servers. You load dozens of tools per server. And before the agent even sees the first user request, you've already burned 50 to 100K tokens just on tool definitions alone. Antropic opens this post with exactly that problem. So the first key sentence that matters for us builders is that agents should discover and load tools on demand, keeping only what's relevant for the current task. This is already a pretty big mindset shift from the original MCP hype. Instead of here's every tool I have, good luck. It's now give the agent a way to search for tools and only materialize the ones that it actually needs right now. And that's the first feature, the tool search tool. So let me read you the core idea in plain language. Instead of loading all tool definitions up front, you simply pass in one special tool called the tool search tool that discovers tools on the go. So this means that Claude doesn't see any of the heavy tool definitions at the start. It only sees the search tool itself, which is only around 500 tokens. Whenever it needs a capability, it calls the search tool with a query like GitHub pull requests or search jer tickets and the search tool returns a small set of matching tools and only those two definitions actually enter the context. So if you have like 200 tools and only three are relevant for this task, you don't pay for all 200 upfront. You only pay for three. They even quantify it. Traditional approach 77k tokens before any work begins with tool search 8.7k tokens preserving 95% of context window and in their internal evals they also see accuracy on mcp tasks jump when using tool search tool because clot is not overwhelmed by a huge list of similar tools that all kind of look the same. To use this new tool search tool in the tools array, you simply add tool search tool regax and then the date when it was released. And for all your other tools that you provided before, you also need to set defer loading to true. And for MCP servers, they even show how to defer the entire server by default and then override only a few hot tools like for example search files to always be loaded, which is honestly a gamecher. So, this is basically MCP plus a builtin router. Many people talked about this in my last video in the comments. Instead of wiring 10 MCP servers directly to your agent and hoping for the best, you now simply register all of them, mark them as deferred, and let Clot search for what it needs when it needs it. The trade-off is simple. You add a search step, so a bit more latency, but you save a lot of context and get better tool selection. My take here is pretty simple. If you have fewer than 10 tools, you probably don't need this yet. If you have multiple MCPS and add 50 plus tools, tool search is a no-brainer. This is the official version of the manual pattern many of us were already hacking together on top of MCP. Next, they also talk about the second problem. Even if you fix the tool definition bloat, we still have the same issues we already discussed in the last video. intermediate tool results polluting the context. So I won't repeat the whole explanation here. Instead, let's focus on what they propose as a solution. Programmatic tool calling. And this is very close to what I showed in the previous video where we talked about code execution with MCP. Instead of making 20 natural language tool calls, let the model write a script that calls your tools in code inside a sandbox. To make this concrete, they walk through a very simple but realistic example. which team members went over their Q3 travel budget. You also have three tools available. Get team members, get expenses, and get budget by level. The traditional way to do this with tool calling is first the agent makes a tool call to get the team members, then it makes tool calls to get the expenses for each team member, then tool calls to get the budgets, and then another model call where it combines all this in its context window. In programmatic tool calling, clot instead writes Python like this. This code runs inside the code execution tool and clot literally calls your other tools or MCP servers inside the Python script. This allows it to process the data inside the script and instead of reading all 2,000 intermediate line items, Claude only sees the final printed JSON. So what does this change in practice? They show three concrete improvements from their internal tests. Token savings. Average token usage dropped 37% on complex research tasks. Reduced latency. Fewer model round trips because one script can handle many tool calls in a single code block and improved accuracy. Explicit loops and conditionals in code are simply more reliable than do this 20 times in natural language. From a builder perspective, this is basically official confirmation that just let your agents run the code is not a hack anymore. It's the recommended approach now. And I'm surprised that Antropic was the first Frontier Model company to do this. But I'm sure that the rest of them will follow. The only difference versus what I showed in the previous MCP video is that now there is a first class API contract around this. Before I had to manually prompt and explain to the model how to do this, how to print two calls and how to work with this new pattern. Now it seems like Claus is literally trained to follow this. So you're not only not wasting your time and tokens for prompting, but it is also more reliable. So the way this works is you simply add code execution to your tools. Then instead of calling your tools directly, Clot writes orchestration code like this. Then you simply execute your code and provide Claude with the final result. This final result is the only thing that Claude sees. It is not seeing the 2,000 expense line items that it processed along the way. However, of course, do not use it with simple agents. Use it with agents that either process large data sets or run multi-step workflows with many different tool calls that need to be combined together. The third feature is tool use examples. In short, this is a way to attach small realistic examples of tool calls directly into the tool definition. So the model sees not just the JSON schema, but also what a good tool call looks like. And this can greatly improve the performance of your agents, especially on tools with many optional fields and conventions. Although to be fair, I don't think that this is something completely new or groundbreaking because even before we could provide examples in parameter descriptions or in a special example field in a JSON schema or in the system prompt here it just becomes a standardized field in the tool definition. So the combined picture is tool search don't load everything only the tools that you need. Programmatic tool calling orchestrate tools in code not in context and tool use examples show the model proper calls not just JSON. This is exactly the direction we've been moving toward manually. The difference is that now Antropic essentially formalized it and gave us a proper APIs to work with. So now the important part with these features it's easier than ever to implement this approach in code because Entropic finally gave us a proper framework to do this. But without proper safeguards, it can actually backfire. There are five main reasons for that. First, this makes your workflow less deterministic. If with traditional MCPS, you are confident that the agent can execute your workflow repeatedly every single time, with tool search and programmatic tool calling, there are more ways it can make a mistake. For example, a tool might not be found in tool search because the query is phrased differently. or the agent might simply write incorrect code in the orchestration script. Second, it's much harder to debug. You will not be able to see the outputs of tools when using tracing platforms in the same way. You'll mainly see the final result coming out of the code. So, if some of the tools are returning wrong data inside that script, you won't see those intermediate tool calls and you might not even know it. Third, it also adds some latency. you're now adding a code execution step and sometimes a search step. For complex multi-step workflows, this is usually worth it, but for very simple single tool tasks, it might actually make things slower compared to the direct MCP tool call. Fourth, it's harder to guarantee safety with this approach. The agent has way more autonomy now. It can write loops, conditionals, and combine multiple tools in ways you didn't explicitly design. The worst case, without proper safeguards, it can even erase your entire tool ecosystem or wipe out all the data if destructive tools are exposed. And fifth, this is also much harder to implement correctly. You do have to set up custom sandboxes where your agents can execute code and all your provided tools safely and store the results. That's non-trivial infrawork if you're not using a platform that already gives you these sandboxes like ours. So, this isn't free power. It's more power with more responsibility. In the next part of the video, I'll show you how I'm already applying the same ideas in my own agent framework with other models and MCP servers that don't even support this in the API yet. So, you'll see how both Anthropic intends this to work and how we actually use these patterns in our own stack safely. If you've never tried building agents with our framework, make sure to follow our getting started guide on our documentation. The process is honestly extremely simple. All you need to do is just copy our starter template and then prompt cursor because this template already contains all of the necessary rules and files that cursor is going to need in order to build your agents for you. I'm going to be converting this called email OS agency that is currently running all cold email campaigns for us and for our clients. And this is a perfect use case for this new pattern because instantly MCP contains I think 36 tools in total and the agent often has to perform 10 or 20 different tool calls in order to complete a task even though it doesn't need to see all the results. So let me show you an example. Let me ask it to find all inboxes with incorrect settings. As you can see, the agent has to perform over 20 different tool calls just to find the accounts that do not have the correct configuration. Even though it could simply iterate through all these accounts in a loop without actually checking the info for each accounts and then only log the accounts that do not have the correct configuration. And then at the end, it provides me with the summary for all the campaign settings for each account. And if we look at the traces, you can see that the final number of tokens consumed is almost 40,000, which is insane. Okay, so now let's jump into the repo and let me show you how to fix this. So we've added a new command called MCP code execution and this command essentially describes to cursor how to use this new pattern. Additionally, we've recently added new methods directly into our framework specifically for this new pattern. So now it's even easier for us to do this than ever before and it's using a very unique approach which I'll explain to you while cursor is doing this. So simply tag this command mcp code execution and tell cursor to convert the campaign manager agent to this new pattern. I recommend selecting at least cloth 4.5 for this. Okay. So first what cursor did is it converted this instantly MCP server into individual tools using this new method that I mentioned earlier and then it created this generate schema file function similar to what anthropic showed in their cookbook. So this generate schema file function now saves the schema inside the campaign manager files directory. So you can see this instantly mcp.txt file with all the tool definitions inside the agent folder. The reason it saves it into the files directory is because this folder is used as the knowledge for the agent. Essentially, all files that you add into this framework will be automatically converted into embeddings and added to your agent's knowledge. Then cursor proceeds to testing at least one tool from each MCP server. And then at the end, it also adds two tools to actually execute code to this agent from our framework and adjusts the agents instructions to use this new pattern. So, as you can see, it literally does everything for you step by step. And finally, it tells me that the conversion is complete. Now, let's deploy our changes. And by the way, this agency is right now deployed on our own platform called Agency AI. And let's wait until the build is finished. Okay, now that our agency is deployed, let's test it again and then compare the traces. So now, as you can see, the agent first searches the files because again, we uploaded all the tool definitions as knowledge to this agent. And then the agent no longer uses the MCP server and instead runs the functions directly in code using this IPython interpreter tool. Then it performed another tool called to actually filter out the inboxes that do not have the correct settings. And then it already provides me with the summary and the final token consumption was much lower only 14,000 tokens this time. So I've been playing with this for the last 2 three weeks and basically all the recent agents that I built are using this MCP code execution approach. The reason is because agents have become just way too good at writing the code. So it makes no sense for us to build the tools for our agents ourselves because they can literally do it better than we can. And so I will be experimenting with this more and more. And I do hope that more providers like Andropic will release this directly in their APIs because this makes it even easier for us builders. The primary challenges with this pattern today is that number one it requires a lot of prompting to explain this to an agent and number two it requires some infrastructure overhead because executing code can sometimes introduce some security risks. So you need to set up secure sandboxes where agents can safely run the code. So if you want to do that check out our platform agency AI and we even provide storage to your agents which means that your agents can also even evolve their own skills. They can save certain files in persistent storage and then reuse them across different chats. That's it for this video.


# Video 4 - https://www.youtube.com/watch?v=9A3azr4JWNo 
All right. What if your AI assistant could act less like a script follower, and more like a really skilled programmer? Today, we're going to dig into a huge shift in how AI agents work. A change that makes them way faster, cheaper, and a whole lot more powerful just by teaching them how to code. So, here's the plan. We're going to start with the big vision of a totally connected AI world, and then we'll shine a light on the huge bottleneck that's been holding everything back. After that, we'll get into the really cool part, an elegant solution. Break down all the surprising perks and wrap it all up with the key things you really need to remember. Okay, so let's jump right in. To really get this, we first have to understand the foundation of how these AI agents connect to the outside world. And that all comes down to a protocol called MCP. So the model context protocol or MCP, just think of it like a universal travel adapter but for AI. It's a standard that lets any AI agent plug into pretty much any tool out there, your Google Drive, Salesforce, you name it, without needing a custom connector for every single one. It's all about universal compatibility. And the goal here is, well, it's massive. We're talking about an AI that can seamlessly tap into thousands of different tools and services. This isn't just about making things a little easier. It's about building agents that are truly powerful and can handle really complex real world tasks. This is how we unlock what they can really do. But there's a pretty big roadblock. You see, as we start plugging in more and more tools, a serious bottleneck starts to show up. It's kind of like a traffic jam inside the agent's short-term memory, which we call its context window. Just picture it like a tiny notepad. If you scribble too much on it, it gets cluttered and the agent can't think straight. You see, the way we do things now by directly calling each tool creates two huge problems. First, the instruction manuals for all the tools, the tool definitions, they just overload the agents memory. And then the results from every single little step, well, that creates even more clutter. What you're left with is a process that's super slow and way more expensive than it should be. And this is the crazy part. Imagine that before you even ask your assistant to do something, it has to read an entire library of user manuals. That's basically what's happening with these tool definitions. Then let's say it goes and gets a meeting transcript. It has to read the whole thing to understand it and then read it all over again to pass it to the next tool. It's just incredibly wasteful. So, how in the world do we fix this giant mess? Well, the solution is actually pretty brilliant. Instead of making the agent call all these tools one by one, we let it do what large language models are amazing at. We let it write code. Just think about that for a second. This one simple shift completely changes how the agent works. It goes from this clunky step-by-step process to something that's way more fluid and efficient. You know, kind of like how a human programmer would actually solve the problem. So, here's what that new smarter workflow looks like. The agent basically acts like a developer. First, it just looks at a list of available tools, you know, like Google Drive or Salesforce. Then, it only reads the instructions for the one specific function it needs, like get a document. And finally, it writes a quick little script to get the job done and just runs it. Simple. And the result of this change, it's honestly, it's staggering. For one sample task, this new method cut down on token usage, time, and the total cost by 98.7%. A job that used to chew through 150,000 tokens now only needs about 2,000. That's a total gamecher. But okay, saving a ton of money and time is awesome. But the benefits here go way beyond just that. This new approach really upgrades what an AI agent can even do in the first place. Okay, first off, with progressive disclosure, the agent only loads the exact tool it needs right when it needs it. Super efficient. Second, it can sift through huge amounts of data on its own and only pass the important summarized bits back to its brain. And finally, it gets the power of real programming logic. You know, using things like loops and if then conditions. And this right here really drives the point home. With the old method, the AI model would have to read all 10,000 rows from a spreadsheet. Just all of it. With code execution, the agent writes a little script to find what's important and only passes the five most relevant rows back to the model. Massive difference. This also unlocks some huge benefits for privacy and security. Really sensitive data can be processed without the AI model itself ever having to see it. The agents can also save their work to a file and pick it up later. And what I think is the most exciting part, they can save the code they write as new reusable skills. they can actually learn and get better over time. So, let's wrap this all up and look at the big picture. This method is incredibly powerful, but like with any really advanced tech, there are some important trade-offs you have to think about. So, on one hand, yeah, the old way with direct tool calls is simpler to set up and it's fine for basic tasks, but for anything complex, code execution is the clear winner. Now, it's not a free lunch. Letting an AI run code means you absolutely need a secure sandboxed environment to keep things safe. So there's a bit more setup and security planning involved up front. So if there's one thing to take away from all this, it's this. That limited working memory. The context window is the biggest thing holding back more powerful AI. The solution is code execution, which basically turns the agent into a little software developer. This shift doesn't just slash cost by almost 99%. It unlocks totally new abilities, making agents more powerful, more private, and able to learn. And what's so cool is that this isn't some crazy new idea. It's just applying smart, proven software engineering principles to solve the next big challenge in AI. Hey, if you found this breakdown helpful and you want more deep dives just like it, do our friendly neighborhood algorithm a little favor. I'm told it gets a tiny dopamine hit every time someone subscribes to Viveoc. So, you know, do it for the AI. Go on and hit subscribe. And that leaves us with one last thing to think about. We've just taught our AI agents how to write their own code to solve problems. As they start to build and save these new skills for themselves, what incredible new things will they teach themselves to do tomorrow?

# Video 5 - Python Sandbox - https://www.youtube.com/watch?v=bm6jegefGyY - https://github.com/pydantic/mcp-run-python#installation
Agents are particularly powerful if you allow them not just to write code but also execute code. The problem is if they execute code on your computer, there's the risk of files being deleted or sensitive information being shared. That's where sandboxing comes in where you create a narrow environment without access to your files and maybe not even to the internet for agents to run code. And I'm going to show you the fastest and most secure ways of doing sandboxing in this video. In brief, one of the fastest and possibly most secure ways to have an agent run code is to use MCP run Python, which is an MCP server that's being developed by Pyantic AI. This sandboxes your code or the agents code in two ways. First of all, it runs it in web assembly and second of all, it runs that via Dino instead of running it via node. And neat dino allows you to control the folders that this agent sandbox has access to and further control whether it has access to a network or not. And because this is run in web assembly, it's extremely fast to spin up and you don't lose time like you would with docker images or by using something like podman. Now in this video I'll go through the types of sandbox. So the main three categories are docker pocket man type approaches which are slower but they can be very secure. the Piodide Dino approach which is very secure but it is limited in that it can run Python and some of the supported libraries in web assembly but it's not able to run say um PyTorch it's not able to support CUDA either and then the third approach which is kind of the small agents approach you can check out my video on small agents where they limit a Python interpreter in a few different ways that make it more secure but it doesn't provide the same level of security as the pi dino approach uh from podantic AI or the approach from docker podman So I'll talk a bit about how uh the small agents approach works. That's the CPython sandbox. I'll talk about how the MCP run Python sandbox works. That's the pod podino. And then I'll give you a demo. I'll show a sandbox uh with a podantic AI agent. So that's running an agent that can run code in the sandbox. And then I'll show you just how to run code in a sandbox. Uh which doesn't need to have an agent. you could just want to run uh some code maybe that you've generated on chatgpt or some other place uh and you want to run it sandboxed. So for all of this I'm going to work out of the advanced inference repo. You can find the scripts on trellis.com/advanced inference. I will describe everything and I'll link below the main open source repos so you can learn and build this for yourself. I have cloned the repo now advanced inference uh locally and I've done that using windsurf and you'll see there's a folder now called sandbox. So I've just moved into that folder and I've opened up the re readme here in um in this readme mode and what I've included is a is a table comparing the three types of sandboxing approach. There's the denopodide, the docker podman and then restricted cython. Now the one I'm going to focus on is dopiodide. This is what I recommend and essentially it is going to convert the Python code into web assembly and then it is going to run it using uh a dino runtime and what that's going to allow you to do because Dino has the feature is to control the folders that it will have access to and also control whether it has network access or not. Now this is quite fast. It should take well less than a second in order for this to work. So quite a bit faster than if you're using Docker or Podman. And this becomes a problem if you're trying to use Docker with an agent that's supposed to execute very quickly. Probably you can even get faster here because it's just going to take uh so much time to to run the image from even if you have it locally. The other approach which I covered in small agents is to take restricted uh CPython approach. By the way, CPython this is um the Python implementation that you all generally use if you install Python on your computer. It's a CPython implementation. Piodide is actually an other implementation of Python and it's one that converts the Python into web assembly which is a very concise form of code that can be run in browsers and just as it can be run in browsers it can be run using node or it can be run using Dino and Dino provides the advantage over node that it's able to control the paths and control the network that it has access to. So that's why Dino is used for running it instead of running node because it's in web assembly. It's very fast and pied is what allows you to go from Python into web assembly. Now I won't dwell on this but just as a very quick example of how you could kind of sandbox using um a restricted CPython interpreter. I'll show you this sample code I've put together here. It's a small agents demo and it shows you a few of the key features that the small agent sandbox uses. The first is an abstract syntax tree. So it will convert it will basically parse your Python code using an abstract syntax tree and then it will go through that tree and check that it adheres to the limited set of libraries and imports that you have set. So basically you have a safe list that you're allowing of imports and libraries and if anything unsafe according to your list is found it's not going to allow uh the code to proceed. Furthermore there's an operations counter. So we'll check that if the number of uh operations goes above a certain amount then it's just going to uh break the loop. So this stops you from having uh certain types of crash because of an overload or some kind of a code bomb. So you can see in the implementation here this is uh the abstract syntax tree gate and it's doing some checks on the white list of imports that are allowed. it's forbidding um certain dangerous attributes and then it's also banning certain commands like exec eval and open. So yeah, this is a very simple implementation. It does provide some levels of protection but it's not going to provide in particular the level of access path uh path access restrictions say to your local folders as you can achieve with piod dino or with a docker podman type approach. Now moving on to explain a little bit how the piodino approach works. I've made this graphic here with chat GPT and you can see C Python. This is the Python implementation you all have in your machines. It's uh written in C code and it will combine compile compile to a native binary that will run on your operating system. And this will have access to a lot of uh commands that are on your say Mac OS operating system. And this means it's not particularly well sandboxed or easy to sandbox. By contrast, piide is going to compile your code into web assembly which is very tur language. It's very compact, runs very fast and that can be uh executed using a dino runtime which is a Java JavaScript runtime that has uh the ability to control permissions and we'll see how to control those permissions and this means it's not going to be able to access folders or even the network if you specify that it cannot. So I'm going to show you an implementation now of running this uh piodide denotype approach. And what I'm going to need is to have installed uh two things. Dino and also I'll actually need to have installed UV. Now you can install UV with just pip install UV if you haven't got it installed. Uh but UV is going to help us to easily run the MCP server uh by specifying uh some supporting packages in in just a oneliner. We'll see that a bit later. So, make sure that you have Dino installed on Mac OS. Um, you can use Homebrew or you can install it uh using curl. Look up the commands if you're on Windows or or Linux. And next, we're going to just initialize uh a project. So, let me open my terminal here. And I'm going to CD into the sandbox folder. And I'll just initialize uh an environment. Now, this is formally adding MCP, which is uh a client. It's adding it as a requirement for anything we run here. Um, but you can also specify it using a width parameter here for example, but I'm going to ensure that we have it defined. So, I'll do UV in it. And yeah, I've already got a project, so it's telling me because I ran that earlier. Now, one of the simplest ways you can run is to set up a pyantic agent. And pyantic agents allow you to pass in MCP servers. So if you create an agent and you pass in an MCP server that is MCP run Python then it's going to allow you to um basically it'll it'll be allowed to access that sandbox for running any types of code. So here you can see a line where the agent is being defined the name of the model is being defined here. Now note that you do need to pass an API key in in advance and then you need to pass in um the server the servers the MCP servers and the server I'm passing in here is an MCP uh ser server operating with uh standard IO and it's being run with Dino as opposed to node because Dino allows for closer access control. Specifically we are allow allowing network access. This is because it needs to fetch some packages. We are then only allowing read access to the node modules folder and only write access to the node modules folder. So there's no access to any other folders on my machine. And we are going to look up the repository and instead of being npm, it's going to be JSOR. That's the dino equivalent uh for npm. So we're looking up JSOR and we're looking for the Pantic MCP run Python. And what this is is a server developed by Pantic AI. And you've got the same uh similar types of commands here, but it runs uh a sandbox that is going to be converting the code into web assembly and then can be run uh using a dino runtime. So once we have the server defined, we've passed it to the agent. We're then able to um start a main function here where we're going to await the agent to run and it's going to run on this request which is to calculate the 10th Fibonacci number. And I'll ask it also to share the program it ran to calculate it and it will have access to this server. So you do need to first add in your entropic API key. So make sure you set that. You can use a different model a different provider if you want say OpenAI. Just swap the model name and swap the API key. Copy paste this here. And then you can uh run like this. So this should be executing in the background and most likely Claude is going to call that sandbox and hopefully use the sandbox to calculate uh the the 10th Fibonacci number. So you can see it's sharing the code here. It's a fairly standard way to calculate and yeah it's able to find uh the 10th number. By the way, sometimes it will give the 10th or 11th depending on whether it counts zero as an actual uh number. So here it's giving the 10th number as 34, which is correct. And it's been able to run this code uh within a sandbox. So that's a very simple agent example. I'll now show you how you can just run some code using the MCP server, but without necessarily having an agent. And to do that, you could define a piece of code. This is just a Python script. You can include uh dependencies if you like uh up here and note that these dependencies are supported uh in web assembly whereas running with uh CUDA that would not be supported. Those libraries also pietorch would not be supported either. So there's a limited set of libraries that you can use if you're going to be in web assembly. That is one of the drawbacks. But here I have uh a script and what I want to do is run this script that's going to create uh some sample data, print some information, print some mean, standard deviations, min and max values. And I just want to run this script. You could be you could put in any script here that's supported uh through piodide. And I want to run it using my sandbox runner. So what my runner is going to do is it's going to set up a server. It's going to use Dino. It's going to allow network access. Read write will only be from the not node modules folder. And then we're going to define a function that's to run this the code within the sandbox. So here we're basically using the standard IO client. We're going to initialize a session. We're going to list the available tools in the session and the session is being initialized from client session which is defined uh by the MCP package. So you can see it's imported up here. And then we're going to await a a tool call. And the tool call is going to be to run Python code using that MCP server. And we'll get back a result here. And I think we're going to print the result. Let's see what we do when we call down here. Yeah, we're going to print the result that we get back from running the runin sandbox function. So I can just uh do UV run sandbox runner. And um this is where it's important that I have got MCP defined within my requirements. You can see it's defined here. I think an alternative would be for me to pass in UV run with MCP and then I could put Cly here like this. And the correct way to do this, I've got the apostrophes in the wrong place here, is UV run with and then wrap like this. MCP cly and then sandboxr runner. py sandbox test py. So I'm passing the script to be run by the runner and that's making available MCP. And now it has run the script and you can see the sandbox execution result is is successful using these dependencies producing these statistics and uh that has worked very well. So there's one more quick example I'll show you which is a direct execution approach very similar to running uh the two uh the script here where I'm running a Python script using the runner but here I'm going to directly execute in a Python interpreter and for the direct execution what you can do is set up a temporary environment with uvx and you can pass in mcp and then python so this will set up a python interpreter now copy all of this code here what's this code doing. It's defining the server using uh it's defining the MCP server. It's defining a code snippet that's just going to print out uh some random values and then it's going to make a call to run that code within the sandbox. So I'll paste it in here, press enter twice, and that should allow the code to run. So here it is. It's returning back the statistics and returning back uh the values here. So, we've successfully run a piece of code just within the Python interpreter using UVX. So, I'll exit now. And that is it for the demo of how to sandbox your code. I think this is a really nice lightweight approach that can allow you to do a lot of code execution and do it on your machine. You can of course use a service like E2B that's fairly popular. You do pay by the second, it will spin up temporary instances. You can get more complex and you can run uh fancier libraries that are not supported uh in web assembly for now. If you really want to use uh deeper packages, for example, you want to use PyTorch, then you may have to use Podman or Docker. So things will be a little slower, but you'll be able to implement more complex code. But for a lot of code execution and given that some of the main libraries like numpy are supported well within the web assembly framework, you should be able to use this pyantic AI type approach in a lot of cases. All right, all the scripts I have put in the advanced inference repo. Any of the links are below in the description and let me know as usual if you've got questions in the comments. Cheers.


